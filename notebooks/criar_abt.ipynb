{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ua5QIqzx9r"
      },
      "source": [
        "# **Case Datarisk - Score de Crédito**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov72IbtKztcb"
      },
      "source": [
        "## **Importa as bibliotecas e Carrega os Dados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Pega o diretório de trabalho atual (que é /notebooks)\n",
        "notebook_dir = os.getcwd()\n",
        "# Sobe um nível para o diretório raiz do projeto\n",
        "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
        "\n",
        "# Adiciona a raiz do projeto ao sys.path se ainda não estiver lá\n",
        "if project_root not in sys.path:\n",
        "    print(f\"Adicionando a raiz do projeto ao path: {project_root}\")\n",
        "    sys.path.append(project_root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: CatBoostEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "psrasy5sn9F5"
      },
      "outputs": [],
      "source": [
        "from src.RankCountVectorizer import RankCountVectorizer\n",
        "from src.load_df import load_df\n",
        "from src.convert_to_datetime import convert_to_datetime\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.metrics import roc_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LoqCUdsKrnS0"
      },
      "outputs": [],
      "source": [
        "# Carregar os DataFrames\n",
        "df_targets = load_df(\"../data/targets.parquet\")\n",
        "\n",
        "df_cadastral = load_df(\"../data/base_cadastral.parquet\")\n",
        "df_emprestimos = load_df(\"../data/historico_emprestimos.parquet\")\n",
        "df_submissao = load_df(\"../data/base_submissao.parquet\")\n",
        "df_parcelas = load_df(\"../data/historico_parcelas.parquet\")\n",
        "dicionario = load_df(\"../data/dicionario_dados.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 71043 entries, 0 to 71042\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   id_contrato   71043 non-null  int64  \n",
            " 1   inadimplente  71043 non-null  float64\n",
            "dtypes: float64(1), int64(1)\n",
            "memory usage: 1.1 MB\n"
          ]
        }
      ],
      "source": [
        "df_targets.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVHS2KAbXYam"
      },
      "source": [
        "## **Converter Datas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v5UeIN7V_LFR",
        "outputId": "9fef84d0-de2b-4494-b5e9-00587ecf7bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Verificando tipos de dados ANTES da conversão ---\n",
            "data_prevista_pagamento    object\n",
            "data_real_pagamento        object\n",
            "dtype: object\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Verificando tipos de dados DEPOIS da conversão ---\n",
            "data_prevista_pagamento    datetime64[ns]\n",
            "data_real_pagamento        datetime64[ns]\n",
            "dtype: object\n",
            "\n",
            "--- Verificando tipos de dados ANTES da conversão ---\n",
            "data_decisao                       object\n",
            "data_liberacao                     object\n",
            "data_primeiro_vencimento           object\n",
            "data_ultimo_vencimento_original    object\n",
            "data_ultimo_vencimento             object\n",
            "data_encerramento                  object\n",
            "dtype: object\n",
            "\n",
            "--- Verificando tipos de dados DEPOIS da conversão ---\n",
            "data_decisao                       datetime64[ns]\n",
            "data_liberacao                     datetime64[ns]\n",
            "data_primeiro_vencimento           datetime64[ns]\n",
            "data_ultimo_vencimento_original    datetime64[ns]\n",
            "data_ultimo_vencimento             datetime64[ns]\n",
            "data_encerramento                  datetime64[ns]\n",
            "dtype: object\n",
            "\n",
            "--- Verificando tipos de dados ANTES da conversão ---\n",
            "data_nascimento    object\n",
            "dtype: object\n",
            "\n",
            "--- Verificando tipos de dados DEPOIS da conversão ---\n",
            "data_nascimento    datetime64[ns]\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# listar as colunas que precisam de conversão em cada DataFrame\n",
        "colunas_data_emprestimos = [\n",
        "    'data_decisao',\n",
        "    'data_liberacao',\n",
        "    'data_primeiro_vencimento',\n",
        "    'data_ultimo_vencimento_original',\n",
        "    'data_ultimo_vencimento',\n",
        "    'data_encerramento'\n",
        "]\n",
        "\n",
        "colunas_data_parcelas = [\n",
        "    'data_prevista_pagamento',\n",
        "    'data_real_pagamento'\n",
        "]\n",
        "\n",
        "colunas_data_cadastral = [\n",
        "    'data_nascimento'\n",
        "]\n",
        "\n",
        "df_parcelas = convert_to_datetime(df_parcelas, colunas_data_parcelas)\n",
        "df_emprestimos = convert_to_datetime(df_emprestimos, colunas_data_emprestimos)\n",
        "df_cadastral = convert_to_datetime(df_cadastral, colunas_data_cadastral)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOCX7HrTSs-1"
      },
      "source": [
        "## **Préprocessamento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr6WjbdQinzD"
      },
      "source": [
        "Extração de Features\n",
        "- Features Cadastrais:\n",
        "  - idade_cliente: Idade do cliente (diferença em anos entre a data atual e a data de nascimento do cliente)\n",
        "  - renda_por_familiar: Quociente entre a renda anual do cliente e a quantidade de membros da sua família\n",
        "  - comprometimento_de_renda: Quociente entre o valor do crédito e a renda anual do cliente\n",
        "- Features históricas:\n",
        "  - num_emp_aceitos_6m: Número de empréstimos solicitados pelo cliente que foram aceitos nos últimos 6 meses\n",
        "  - atraso_medio: Média do número de dias atrasados por cliente até o momento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 40000 entries, 0 to 39999\n",
            "Data columns (total 16 columns):\n",
            " #   Column                      Non-Null Count  Dtype         \n",
            "---  ------                      --------------  -----         \n",
            " 0   id_cliente                  40000 non-null  int64         \n",
            " 1   sexo                        40000 non-null  object        \n",
            " 2   data_nascimento             40000 non-null  datetime64[ns]\n",
            " 3   qtd_filhos                  40000 non-null  int64         \n",
            " 4   qtd_membros_familia         40000 non-null  float64       \n",
            " 5   renda_anual                 40000 non-null  float64       \n",
            " 6   tipo_renda                  40000 non-null  object        \n",
            " 7   ocupacao                    27324 non-null  object        \n",
            " 8   tipo_organizacao            40000 non-null  object        \n",
            " 9   nivel_educacao              40000 non-null  object        \n",
            " 10  estado_civil                40000 non-null  object        \n",
            " 11  tipo_moradia                40000 non-null  object        \n",
            " 12  possui_carro                40000 non-null  object        \n",
            " 13  possui_imovel               40000 non-null  object        \n",
            " 14  nota_regiao_cliente         40000 non-null  int64         \n",
            " 15  nota_regiao_cliente_cidade  40000 non-null  int64         \n",
            "dtypes: datetime64[ns](1), float64(2), int64(4), object(9)\n",
            "memory usage: 4.9+ MB\n"
          ]
        }
      ],
      "source": [
        "df_cadastral.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "li2nUTJbcVIP"
      },
      "outputs": [],
      "source": [
        "data_atual_idade = pd.to_datetime('today')\n",
        "df_cadastral['idade_cliente'] = ((data_atual_idade - df_cadastral['data_nascimento']).dt.days / 365.25).astype(int)\n",
        "\n",
        "df_cadastral = df_cadastral.drop(columns=['data_nascimento', 'sexo'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uzeD5sI_qXYQ"
      },
      "outputs": [],
      "source": [
        "df_cadastral['reda_por_familiar'] = df_cadastral['renda_anual'] / df_cadastral['qtd_membros_familia']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wftrrRuzTphj"
      },
      "outputs": [],
      "source": [
        "include_columns = [\n",
        "                  'id_contrato',\n",
        "                  'id_cliente',\n",
        "                  'dia_semana_solicitacao',\n",
        "                  'hora_solicitacao',\n",
        "                  'tipo_contrato',\n",
        "                  'valor_credito',\n",
        "                  'valor_bem',\n",
        "                  'valor_parcela',\n",
        "                  'data_decisao'\n",
        "                  ]\n",
        "\n",
        "df = df_emprestimos.copy()\n",
        "df = df[include_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2D5kB-VbUt20"
      },
      "outputs": [],
      "source": [
        "df_merged = df.merge(df_cadastral, on='id_cliente', how='left')\n",
        "df_merged = df_merged.merge(df_targets, on='id_contrato', how='right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Timestamp('2024-05-04 00:00:00')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_merged['data_decisao'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removidos 10182 registros.\n",
            "Novo período: de 2018-08-01 até 2024-05-04 (60861 registros)\n"
          ]
        }
      ],
      "source": [
        "# Informações iniciais\n",
        "original_count = len(df_merged)\n",
        "\n",
        "cutoff_str = \"01/08/2018\"\n",
        "cutoff_date = pd.to_datetime(cutoff_str, format='%d/%m/%Y')\n",
        "\n",
        "df_merged = df_merged[df_merged['data_decisao'] >= cutoff_date].copy()\n",
        "\n",
        "removed_count = original_count - len(df_merged)\n",
        "\n",
        "if len(df_merged) == 0:\n",
        "    print(f\"Foram removidos {removed_count} registros. O DataFrame resultante está vazio.\")\n",
        "else:\n",
        "    print(f\"Removidos {removed_count} registros.\")\n",
        "    print(f\"Novo período: de {df_merged['data_decisao'].min().date()} até {df_merged['data_decisao'].max().date()} ({len(df_merged)} registros)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 60861 entries, 10182 to 71042\n",
            "Data columns (total 25 columns):\n",
            " #   Column                      Non-Null Count  Dtype         \n",
            "---  ------                      --------------  -----         \n",
            " 0   id_contrato                 60861 non-null  int64         \n",
            " 1   id_cliente                  60861 non-null  int64         \n",
            " 2   dia_semana_solicitacao      60861 non-null  object        \n",
            " 3   hora_solicitacao            60861 non-null  int64         \n",
            " 4   tipo_contrato               60861 non-null  object        \n",
            " 5   valor_credito               60861 non-null  float64       \n",
            " 6   valor_bem                   58605 non-null  float64       \n",
            " 7   valor_parcela               60860 non-null  float64       \n",
            " 8   data_decisao                60861 non-null  datetime64[ns]\n",
            " 9   qtd_filhos                  60861 non-null  int64         \n",
            " 10  qtd_membros_familia         60861 non-null  float64       \n",
            " 11  renda_anual                 60861 non-null  float64       \n",
            " 12  tipo_renda                  60861 non-null  object        \n",
            " 13  ocupacao                    40772 non-null  object        \n",
            " 14  tipo_organizacao            60861 non-null  object        \n",
            " 15  nivel_educacao              60861 non-null  object        \n",
            " 16  estado_civil                60861 non-null  object        \n",
            " 17  tipo_moradia                60861 non-null  object        \n",
            " 18  possui_carro                60861 non-null  object        \n",
            " 19  possui_imovel               60861 non-null  object        \n",
            " 20  nota_regiao_cliente         60861 non-null  int64         \n",
            " 21  nota_regiao_cliente_cidade  60861 non-null  int64         \n",
            " 22  idade_cliente               60861 non-null  int64         \n",
            " 23  reda_por_familiar           60861 non-null  float64       \n",
            " 24  inadimplente                60861 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(7), int64(7), object(10)\n",
            "memory usage: 12.1+ MB\n"
          ]
        }
      ],
      "source": [
        "df_merged.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from itertools import combinations\n",
        "\n",
        "class FeatureAutomator:\n",
        "    \"\"\"\n",
        "    Versão que inclui um imputer categórico e o RankCountVectorizer.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_cols, cat_cols, date_col):\n",
        "        self.num_cols = num_cols\n",
        "        self.cat_cols = cat_cols\n",
        "        self.date_col = date_col\n",
        "        self.poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
        "        self.num_col_pairs = list(combinations(num_cols, 2))\n",
        "        self.cat_col_pairs = list(combinations(cat_cols, 2))\n",
        "        \n",
        "        # Imputers para numéricos e categóricos\n",
        "        self.num_imputer = SimpleImputer(strategy='median')\n",
        "        self.cat_imputer = SimpleImputer(strategy='constant', fill_value='NA') \n",
        "        \n",
        "        # Encoder Categórico\n",
        "        self.rank_count_vectorizer = RankCountVectorizer() \n",
        "        \n",
        "        self.poly_feature_names_ = None\n",
        "\n",
        "    def fit(self, df):\n",
        "        \"\"\"\n",
        "        Ajusta todos os transformadores (imputers, encoder, poly) nos dados de treino.\n",
        "        \"\"\"\n",
        "        print(\"Ajustando (fit) os transformadores...\")\n",
        "        \n",
        "        self.num_imputer.fit(df[self.num_cols])\n",
        "        self.cat_imputer.fit(df[self.cat_cols])\n",
        "        \n",
        "        # Ajusta o RankCountVectorizer nos dados categóricos já imputados\n",
        "        df_cat_imputed = pd.DataFrame(self.cat_imputer.transform(df[self.cat_cols]), columns=self.cat_cols, index=df.index)\n",
        "        self.rank_count_vectorizer.fit(df_cat_imputed, cols=self.cat_cols)\n",
        "        \n",
        "        # transforma com o imputer (gera um array NumPy)\n",
        "        df_num_imputed_np = self.num_imputer.transform(df[self.num_cols])\n",
        "        # reconstrói o DataFrame para manter os nomes das colunas\n",
        "        df_num_imputed_df = pd.DataFrame(df_num_imputed_np, columns=self.num_cols, index=df.index)\n",
        "        # ajusta o PolynomialFeatures no DataFrame com nomes\n",
        "        self.poly.fit(df_num_imputed_df)\n",
        "        \n",
        "        self.poly_feature_names_ = self.poly.get_feature_names_out(self.num_cols)\n",
        "        self.interaction_feature_names_ = [name for name in self.poly_feature_names_ if ' ' in name]\n",
        "        \n",
        "        print(\"Ajuste concluído.\")\n",
        "        return self\n",
        "\n",
        "    def transform(self, df):\n",
        "        \"\"\"Aplica todas as transformações de features.\"\"\"\n",
        "        df_transformed = df.copy()\n",
        "        print(\"\\nIniciando a transformação (transform)...\")\n",
        "\n",
        "        # ETAPA 0: Imputação de dados numéricos e categóricos\n",
        "        print(\"0. Aplicando imputação de valores ausentes...\")\n",
        "        df_transformed[self.num_cols] = self.num_imputer.transform(df_transformed[self.num_cols])\n",
        "        df_transformed[self.cat_cols] = self.cat_imputer.transform(df_transformed[self.cat_cols]) # NOVO\n",
        "\n",
        "        # As features de interação e frequência devem ser criadas ANTES do encoding final\n",
        "        print(\"1. Criando features de data...\")\n",
        "        df_transformed = self._create_date_features(df_transformed)\n",
        "        print(\"2. Criando features de razão numérica...\")\n",
        "        df_transformed = self._create_numerical_ratios(df_transformed)\n",
        "        print(\"3. Criando features de interação numérica...\")\n",
        "        df_transformed = self._create_polynomial_features(df_transformed)\n",
        "        print(\"4. Aplicando RankCountVectorizer nas colunas categóricas originais...\")\n",
        "        df_transformed = self.rank_count_vectorizer.transform(df_transformed, cols=self.cat_cols) # NOVO\n",
        "\n",
        "        print(\"Transformação de features concluída!\")\n",
        "        return df_transformed\n",
        "\n",
        "    # O resto das funções internas permanecem as mesmas...\n",
        "    def _create_date_features(self, df):\n",
        "        if self.date_col in df.columns:\n",
        "            df[f'{self.date_col}_ano'] = df[self.date_col].dt.year\n",
        "            df[f'{self.date_col}_mes'] = df[self.date_col].dt.month\n",
        "            # garantir que a coluna é datetime antes de extrair componentes\n",
        "            df[self.date_col] = pd.to_datetime(df[self.date_col], errors='coerce')\n",
        "            df[f'{self.date_col}_dia_do_mes'] = df[self.date_col].dt.day\n",
        "            df = df.drop(columns=[self.date_col])\n",
        "        return df\n",
        "    \n",
        "    def _create_numerical_ratios(self, df):\n",
        "        if 'valor_credito' in df.columns and 'renda_anual' in df.columns:\n",
        "            df['ratio_credito_renda'] = df['valor_credito'] / (df['renda_anual'] + 1e-6)\n",
        "        return df\n",
        "\n",
        "    def _create_polynomial_features(self, df):\n",
        "        interactions = self.poly.transform(df[self.num_cols])\n",
        "        df_interactions = pd.DataFrame(interactions, columns=self.poly_feature_names_, index=df.index)\n",
        "        for col in self.interaction_feature_names_:\n",
        "            new_col_name = 'inter_' + col.replace(' ', '_x_')\n",
        "            df[new_col_name] = df_interactions[col]\n",
        "        return df\n",
        "    \n",
        "    import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_historical_features(df, num_cols, date_col='data_decisao', time_windows_months=[3, 6], aggs=['mean','std','min','max','sum']):\n",
        "\n",
        "    print(\"Iniciando criação de features históricas...\")\n",
        "    \n",
        "    # 1. Preparar o DataFrame base, garantindo a data e um índice único\n",
        "    base_df = df[['id_cliente', date_col] + num_cols].copy()\n",
        "    base_df[date_col] = pd.to_datetime(base_df[date_col])\n",
        "    # Usar o índice original como um identificador único para cada empréstimo\n",
        "    base_df = base_df.reset_index().rename(columns={'index': 'id_original_emprestimo'})\n",
        "\n",
        "    # 2. O Self-Join: Juntar o DataFrame com ele mesmo no 'id_cliente'\n",
        "    # 'df_atual' representa cada empréstimo individualmente (lado esquerdo)\n",
        "    # 'df_hist' representa todo o histórico disponível para aquele contrato (lado direito)\n",
        "    df_merged = pd.merge(\n",
        "        base_df,\n",
        "        base_df,\n",
        "        on='id_cliente',\n",
        "        suffixes=('_atual', '_hist')\n",
        "    )\n",
        "\n",
        "    # 3. Filtro Temporal: Manter apenas registros onde o histórico é ANTERIOR ao atual\n",
        "    df_merged = df_merged[df_merged[f'{date_col}_hist'] < df_merged[f'{date_col}_atual']].copy()\n",
        "\n",
        "    # 4. Calcular a diferença em dias entre o empréstimo atual e cada um do seu histórico\n",
        "    df_merged['time_diff_days'] = (df_merged[f'{date_col}_atual'] - df_merged[f'{date_col}_hist']).dt.days\n",
        "\n",
        "    df_final = df.copy()\n",
        "\n",
        "    # 5. Loop para cada janela de tempo para agregar e juntar os resultados\n",
        "    for w in time_windows_months:\n",
        "        window_days = int(w * 30)\n",
        "        print(f\"Calculando para a janela de {w} meses ({window_days} dias)...\")\n",
        "        \n",
        "        # Filtra o histórico para a janela de tempo específica\n",
        "        df_window = df_merged[df_merged['time_diff_days'] <= window_days]\n",
        "\n",
        "        # Mapeia os nomes das colunas de histórico (ex: 'valor_credito_hist')\n",
        "        hist_cols = [f'{col}_hist' for col in num_cols]\n",
        "        \n",
        "        # Agrupa pelo ID do empréstimo atual e calcula as estatísticas sobre o histórico\n",
        "        agg_result = df_window.groupby('id_original_emprestimo_atual')[hist_cols].agg(aggs)\n",
        "        \n",
        "        # Achata os nomes das colunas do MultiIndex (ex: ('valor_credito_hist', 'mean'))\n",
        "        agg_result.columns = [f\"hist_{col.replace('_hist', '')}_{w}m_{agg}\" for col, agg in agg_result.columns]\n",
        "        \n",
        "        # Junta os resultados de volta ao DataFrame final\n",
        "        df_final = df_final.join(agg_result)\n",
        "\n",
        "    # Preenche NaNs que surgem (ex: primeiro empréstimo de um cliente não tem histórico)\n",
        "    hist_feature_cols = [c for c in df_final.columns if isinstance(c, str) and c.startswith('hist_')]\n",
        "    df_final[hist_feature_cols] = df_final[hist_feature_cols].fillna(0)\n",
        "    \n",
        "    print(f\"\\nCriadas {len(hist_feature_cols)} features históricas para as janelas {time_windows_months} meses.\")\n",
        "    \n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_special_features(df, df_emprestimos, df_parcelas, time_windows_months=[3, 6]):\n",
        "    \"\"\"\n",
        "    Versão vetorizada e de alta performance para criar features de janela temporal fixa.\n",
        "    - Utiliza self-joins temporais para evitar o uso de .iterrows().\n",
        "    \"\"\"\n",
        "    df_featured = df.copy()\n",
        "    df_featured['data_decisao'] = pd.to_datetime(df_featured['data_decisao'])\n",
        "    \n",
        "    time_windows_days = [w * 30 for w in time_windows_months]\n",
        "\n",
        "    # --- 1. Cálculo de Empréstimos Aceitos (Vetorizado) ---\n",
        "    print(\"Calculando o número de empréstimos aceitos...\")\n",
        "    \n",
        "    # Prepara o histórico de empréstimos\n",
        "    hist_emprestimos = df_emprestimos[df_emprestimos['status_contrato'].isin(['Approved', 'Aprovado'])][['id_cliente', 'data_decisao']].copy()\n",
        "    hist_emprestimos.rename(columns={'data_decisao': 'data_decisao_hist'}, inplace=True)\n",
        "    hist_emprestimos['data_decisao_hist'] = pd.to_datetime(hist_emprestimos['data_decisao_hist'])\n",
        "\n",
        "    # Self-join temporal\n",
        "    merged_emprestimos = pd.merge(df_featured[['id_cliente', 'data_decisao']], hist_emprestimos, on='id_cliente')\n",
        "    merged_emprestimos = merged_emprestimos[merged_emprestimos['data_decisao_hist'] < merged_emprestimos['data_decisao']]\n",
        "    merged_emprestimos['time_diff'] = (merged_emprestimos['data_decisao'] - merged_emprestimos['data_decisao_hist']).dt.days\n",
        "\n",
        "    # Cria colunas de contagem para cada janela\n",
        "    for window_days in time_windows_days:\n",
        "        window_months = int(window_days / 30)\n",
        "        col_name = f'num_emp_aceitos_{window_months}m'\n",
        "        # Conta condicionalmente usando a soma de uma máscara booleana\n",
        "        counts = merged_emprestimos[merged_emprestimos['time_diff'] <= window_days].groupby('data_decisao').size()\n",
        "        df_featured[col_name] = df_featured['data_decisao'].map(counts).fillna(0).astype(int)\n",
        "\n",
        "    # --- 2. Cálculo de Atraso Médio (Vetorizado) ---\n",
        "    print(\"Calculando o atraso médio...\")\n",
        "\n",
        "    # Prepara o histórico de parcelas\n",
        "    hist_parcelas = df_parcelas[['id_contrato', 'data_prevista_pagamento', 'data_real_pagamento']].copy()\n",
        "    hist_parcelas['data_prevista_pagamento'] = pd.to_datetime(hist_parcelas['data_prevista_pagamento'])\n",
        "    hist_parcelas['data_real_pagamento'] = pd.to_datetime(hist_parcelas['data_real_pagamento'])\n",
        "    hist_parcelas['atraso'] = (hist_parcelas['data_real_pagamento'] - hist_parcelas['data_prevista_pagamento']).dt.days.clip(lower=0)\n",
        "    hist_parcelas = pd.merge(hist_parcelas, df_emprestimos[['id_contrato', 'id_cliente']], on='id_contrato', how='left')\n",
        "    hist_parcelas.rename(columns={'data_prevista_pagamento': 'data_prevista_hist'}, inplace=True)\n",
        "\n",
        "    # Self-join temporal com as parcelas\n",
        "    merged_parcelas = pd.merge(df_featured[['id_cliente', 'data_decisao']], hist_parcelas, on='id_cliente')\n",
        "    merged_parcelas = merged_parcelas[merged_parcelas['data_prevista_hist'] < merged_parcelas['data_decisao']]\n",
        "    merged_parcelas['time_diff'] = (merged_parcelas['data_decisao'] - merged_parcelas['data_prevista_hist']).dt.days\n",
        "    \n",
        "    # Calcula a média condicional para cada janela\n",
        "    for window_days in time_windows_days:\n",
        "        window_months = int(window_days / 30)\n",
        "        col_name = f'atraso_medio_{window_months}m'\n",
        "        \n",
        "        # Usa np.where para aplicar a condição da janela\n",
        "        merged_parcelas['atraso_na_janela'] = np.where(merged_parcelas['time_diff'] <= window_days, merged_parcelas['atraso'], np.nan)\n",
        "        \n",
        "        # Calcula a média, que ignora NaNs\n",
        "        avg_delay = merged_parcelas.groupby('data_decisao')['atraso_na_janela'].mean()\n",
        "        df_featured[col_name] = df_featured['data_decisao'].map(avg_delay).fillna(0)\n",
        "\n",
        "    return df_featured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_features = ['valor_credito', 'valor_bem', 'valor_parcela']\n",
        "\n",
        "categorical_features = df_merged.select_dtypes(include=['object']).columns.tolist()\n",
        "date_feature = 'data_decisao'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculando o número de empréstimos aceitos...\n",
            "Calculando o atraso médio...\n"
          ]
        }
      ],
      "source": [
        "df_featured = create_special_features(df_merged, df_emprestimos, df_parcelas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando criação de features históricas...\n",
            "Calculando para a janela de 3 meses (90 dias)...\n",
            "Calculando para a janela de 6 meses (180 dias)...\n",
            "\n",
            "Criadas 30 features históricas para as janelas [3, 6] meses.\n"
          ]
        }
      ],
      "source": [
        "df_featured = create_historical_features(df_featured, num_cols=numerical_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Período de Treino: de 2018-08-01 até 2023-01-20 (42602 amostras)\n",
            "Período de Validação: de 2023-01-20 até 2023-07-12 (9129 amostras)\n",
            "Período de Teste: de 2023-07-12 até 2024-05-04 (9130 amostras)\n"
          ]
        }
      ],
      "source": [
        "df_featured = df_featured.drop(columns=['id_contrato', 'id_cliente'])\n",
        "\n",
        "train_end = int(len(df_featured) * 0.70)\n",
        "validation_end = int(len(df_featured) * 0.85)\n",
        "\n",
        "# fazer a divisão\n",
        "df_train = df_featured.iloc[:train_end]\n",
        "df_val = df_featured.iloc[train_end:validation_end]\n",
        "df_test = df_featured.iloc[validation_end:]\n",
        "\n",
        "print(f\"Período de Treino: de {df_train['data_decisao'].min().date()} até {df_train['data_decisao'].max().date()} ({len(df_train)} amostras)\")\n",
        "print(f\"Período de Validação: de {df_val['data_decisao'].min().date()} até {df_val['data_decisao'].max().date()} ({len(df_val)} amostras)\")\n",
        "print(f\"Período de Teste: de {df_test['data_decisao'].min().date()} até {df_test['data_decisao'].max().date()} ({len(df_test)} amostras)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ajustando (fit) os transformadores...\n",
            "Ajuste concluído.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iniciando a transformação (transform)...\n",
            "0. Aplicando imputação de valores ausentes...\n",
            "1. Criando features de data...\n",
            "2. Criando features de razão numérica...\n",
            "3. Criando features de interação numérica...\n",
            "4. Aplicando RankCountVectorizer nas colunas categóricas originais...\n",
            "Transformação de features concluída!\n",
            "\n",
            "Iniciando a transformação (transform)...\n",
            "0. Aplicando imputação de valores ausentes...\n",
            "1. Criando features de data...\n",
            "2. Criando features de razão numérica...\n",
            "3. Criando features de interação numérica...\n",
            "4. Aplicando RankCountVectorizer nas colunas categóricas originais...\n",
            "Transformação de features concluída!\n",
            "\n",
            "Iniciando a transformação (transform)...\n",
            "0. Aplicando imputação de valores ausentes...\n",
            "1. Criando features de data...\n",
            "2. Criando features de razão numérica...\n",
            "3. Criando features de interação numérica...\n",
            "4. Aplicando RankCountVectorizer nas colunas categóricas originais...\n",
            "Transformação de features concluída!\n"
          ]
        }
      ],
      "source": [
        "feature_factory = FeatureAutomator(\n",
        "    num_cols=numerical_features,\n",
        "    cat_cols=categorical_features,\n",
        "    date_col=date_feature\n",
        ")\n",
        "\n",
        "# AJUSTE (FIT) a fábrica de features APENAS com os dados de TREINO\n",
        "feature_factory.fit(df_train)\n",
        "\n",
        "# TRANSFORME ambos os conjuntos, treino e teste\n",
        "df_train = feature_factory.transform(df_train)\n",
        "df_test = feature_factory.transform(df_test)\n",
        "df_val = feature_factory.transform(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 42602 entries, 10182 to 52783\n",
            "Empty DataFrame\n",
            "Não há colunas categóricas.\n"
          ]
        }
      ],
      "source": [
        "cat_cols = df_train.select_dtypes(include=['object']).info()\n",
        "\n",
        "if cat_cols is None:\n",
        "    print(\"Não há colunas categóricas.\")\n",
        "else:\n",
        "    print(\"Colunas categóricas presentes: \", cat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total de valores ausentes no conjunto de treino: 0\n",
            "Total de valores ausentes no conjunto de teste: 0\n"
          ]
        }
      ],
      "source": [
        "count_train_na = df_train.isna().sum().sum()\n",
        "count_test_na = df_test.isna().sum().sum()\n",
        "\n",
        "print(\"Total de valores ausentes no conjunto de treino:\", count_train_na)\n",
        "print(\"Total de valores ausentes no conjunto de teste:\", count_test_na)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dimensões do X_train com novas features: (42602, 63)\n",
            "Dimensões do X_test com novas features: (9130, 63)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDimensões do X_train com novas features:\", df_train.shape)\n",
        "print(\"Dimensões do X_test com novas features:\", df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dia_semana_solicitacao                   int64\n",
            "hora_solicitacao                         int64\n",
            "tipo_contrato                            int64\n",
            "valor_credito                          float64\n",
            "valor_bem                              float64\n",
            "valor_parcela                          float64\n",
            "qtd_filhos                               int64\n",
            "qtd_membros_familia                    float64\n",
            "renda_anual                            float64\n",
            "tipo_renda                               int64\n",
            "ocupacao                                 int64\n",
            "tipo_organizacao                         int64\n",
            "nivel_educacao                           int64\n",
            "estado_civil                             int64\n",
            "tipo_moradia                             int64\n",
            "possui_carro                             int64\n",
            "possui_imovel                            int64\n",
            "nota_regiao_cliente                      int64\n",
            "nota_regiao_cliente_cidade               int64\n",
            "idade_cliente                            int64\n",
            "reda_por_familiar                      float64\n",
            "inadimplente                           float64\n",
            "num_emp_aceitos_3m                       int64\n",
            "num_emp_aceitos_6m                       int64\n",
            "atraso_medio_3m                        float64\n",
            "atraso_medio_6m                        float64\n",
            "hist_valor_credito_3m_mean             float64\n",
            "hist_valor_credito_3m_std              float64\n",
            "hist_valor_credito_3m_min              float64\n",
            "hist_valor_credito_3m_max              float64\n",
            "hist_valor_credito_3m_sum              float64\n",
            "hist_valor_bem_3m_mean                 float64\n",
            "hist_valor_bem_3m_std                  float64\n",
            "hist_valor_bem_3m_min                  float64\n",
            "hist_valor_bem_3m_max                  float64\n",
            "hist_valor_bem_3m_sum                  float64\n",
            "hist_valor_parcela_3m_mean             float64\n",
            "hist_valor_parcela_3m_std              float64\n",
            "hist_valor_parcela_3m_min              float64\n",
            "hist_valor_parcela_3m_max              float64\n",
            "hist_valor_parcela_3m_sum              float64\n",
            "hist_valor_credito_6m_mean             float64\n",
            "hist_valor_credito_6m_std              float64\n",
            "hist_valor_credito_6m_min              float64\n",
            "hist_valor_credito_6m_max              float64\n",
            "hist_valor_credito_6m_sum              float64\n",
            "hist_valor_bem_6m_mean                 float64\n",
            "hist_valor_bem_6m_std                  float64\n",
            "hist_valor_bem_6m_min                  float64\n",
            "hist_valor_bem_6m_max                  float64\n",
            "hist_valor_bem_6m_sum                  float64\n",
            "hist_valor_parcela_6m_mean             float64\n",
            "hist_valor_parcela_6m_std              float64\n",
            "hist_valor_parcela_6m_min              float64\n",
            "hist_valor_parcela_6m_max              float64\n",
            "hist_valor_parcela_6m_sum              float64\n",
            "data_decisao_ano                         int32\n",
            "data_decisao_mes                         int32\n",
            "data_decisao_dia_do_mes                  int32\n",
            "ratio_credito_renda                    float64\n",
            "inter_valor_credito_x_valor_bem        float64\n",
            "inter_valor_credito_x_valor_parcela    float64\n",
            "inter_valor_bem_x_valor_parcela        float64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(df_train.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train.to_parquet(\"../data/train.parquet\", index=False)\n",
        "df_test.to_parquet(\"../data/test.parquet\", index=False)\n",
        "df_val.to_parquet(\"../data/validation.parquet\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zajQ48NDOIAk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "case-ds (3.13.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
